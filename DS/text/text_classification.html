
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>An overview of text classification &#8212; Ph.D. | Sr. Data Scientist</title>
    
  <link rel="stylesheet" href="../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Time Series Analysis" href="../timeseries/timeseries.html" />
    <link rel="prev" title="Text analysis basics in Python" href="text_basics.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/name.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Ph.D. | Sr. Data Scientist</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   Welcome
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Data Science
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../basics/basics.html">
   Basics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/git.html">
     Git workflow for data scientist
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/conda.html">
     Conda environment 101
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dataaccess/dataaccess.html">
   Data Accessing
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../etl/etl.html">
   ETL pipeline
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../etl/airflow_bigquery.html">
     Airflow with Google BigQuery and Slack
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../visualization/visualization.html">
   Visualization
  </a>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="text.html">
   Text Analysis
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="text_basics.html">
     Text analysis basics in Python
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     An overview of text classification
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../timeseries/timeseries.html">
   Time Series Analysis
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../timeseries/timeseries1.html">
     Time series analysis using Prophet in Python — Part 1: Math explained
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../timeseries/timeseries2.html">
     Time series analysis using Prophet in Python — Part 2: Hyperparameter Tuning and Cross Validation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../survival/survival.html">
   Survival Analysis
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../survival/survivalanalysis.html">
     Survival analysis using lifelines in Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../survival/survivalanalysis.html#kaplan-meiser-estimate">
     Kaplan-Meiser Estimate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../survival/survivalanalysis.html#nelson-aalen-estimate">
     Nelson Aalen Estimate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../survival/survivalanalysis.html#exponential-model">
     Exponential model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../survival/survivalanalysis.html#weibull-model">
     Weibull model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../survival/survivalanalysis.html#survival-regression">
     Survival regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../survival/survivalanalysis.html#id2">
     Model selection
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/machinelearning.html">
   Machine Learning
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../deeplearning/deeplearning.html">
   Deep Learning
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../deeplearning/input_normalization.html">
     Deep learning basics — input normalization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deeplearning/batch_normalization.html">
     Deep learning basics — batch normalization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deeplearning/weight_decay.html">
     Deep learning basics — weight decay
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deeplearning/data_augmentation.html">
     Deep learning basics — data augmentation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../othermodels/othermodels.html">
   Other Models
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../othermodels/price_sensitivity.html">
     Pricing research — Van Westendorp’s Price Sensitivity Meter in Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../othermodels/clv.html">
     Customer lifetime value in a discrete-time contractual setting
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  random
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../random/pypowerup.html">
   PyPowerUp
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../markdown.html">
   Markdown template
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../notebooks.html">
   Notebooks template
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/DS/text/text_classification.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-preparation-and-data-exploration">
   Data Preparation and Data Exploration
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word-embedding">
     Word Embedding
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bag-of-words">
       Bag of words
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#word2vec-embedding">
       Word2vec Embedding
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#character-embedding">
       Character Embedding
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#n-gram-analysis">
     N-gram analysis
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#topic-modeling">
     Topic modeling
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#labeling">
   Labeling
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modeling">
   Modeling
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#machine-learning-model">
     Machine learning model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deep-learning-model">
     Deep learning model
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="an-overview-of-text-classification">
<h1>An overview of text classification<a class="headerlink" href="#an-overview-of-text-classification" title="Permalink to this headline">¶</a></h1>
<p><em>How to approach a text classification problem</em></p>
<p><img alt="" src="../../_images/text_classification0.png" />
<em><a class="reference external" href="https://unsplash.com/photos/8EzNkvLQosk">source</a></em></p>
<p>Imagine we have a large number of text files and we need to classify these text files into different topics. What should we do? This article will walk you through an overview of text classification and how I would approach this problem on a high-level basis. I would like to address this problem in three steps — data preparation and exploration, labeling, and modeling.</p>
<div class="section" id="data-preparation-and-data-exploration">
<h2>Data Preparation and Data Exploration<a class="headerlink" href="#data-preparation-and-data-exploration" title="Permalink to this headline">¶</a></h2>
<p>The first step is data preparation and exploration. I will transform our text data into a matrix representation through different word embedding methods. Then, I will perform an N-gram analysis and topic modeling to explore the data in more detail.</p>
<div class="section" id="word-embedding">
<h3>Word Embedding<a class="headerlink" href="#word-embedding" title="Permalink to this headline">¶</a></h3>
<div class="section" id="bag-of-words">
<h4>Bag of words<a class="headerlink" href="#bag-of-words" title="Permalink to this headline">¶</a></h4>
<p>Most text analysis and machine learning models use the bag of words embedding, which tokenizes our text into tokens, normalize tokens, count occurrences, apply weights (optional), filter out stopwords, and create a document-term matrix. Bag of words assumes the independence of the words and does not take into account the sequence of the words. I will use the bag of words approach for my data exploration and machine learning models.</p>
</div>
<div class="section" id="word2vec-embedding">
<h4>Word2vec Embedding<a class="headerlink" href="#word2vec-embedding" title="Permalink to this headline">¶</a></h4>
<p>Deep learning models often use the pre-trained word2vec embeddings, which incorporate the information of word similarities. The advantage of word2vec is that it has much fewer dimensions than the bag of words approach, and our document-term matrix will be a dense matrix, and not sparse. I plan to use a word2vec embedding (e.g., word2vec-google-news-300) for my deep learning models.</p>
</div>
<div class="section" id="character-embedding">
<h4>Character Embedding<a class="headerlink" href="#character-embedding" title="Permalink to this headline">¶</a></h4>
<p>Some deep learning models use character embedding and build models at the character-level directly [1][2]. Characters can include English characters, digits, special characters, and others. The advantage of character embedding is that it can model with uncommon words and unknown words. I might try character embedding with my deep learning models to compare with the word2vec embedding.</p>
</div>
</div>
<div class="section" id="n-gram-analysis">
<h3>N-gram analysis<a class="headerlink" href="#n-gram-analysis" title="Permalink to this headline">¶</a></h3>
<p>With the bag of words approach, we can investigate the single word (unigram), and combinations of two words and three words (Bigram/Trigram). With N-gram analysis, we can have a descriptive view of which words or word combinations are being used the most.</p>
</div>
<div class="section" id="topic-modeling">
<h3>Topic modeling<a class="headerlink" href="#topic-modeling" title="Permalink to this headline">¶</a></h3>
<p>Next is topic modeling. There are two ways to do topic modeling: NMF models and LDA models. Non-Negative Matrix Factorization (NMF) is a matrix decomposition method, which decomposes a matrix into the product of W and H of non-negative elements. The default method optimizes the distance between the original matrix and WH, i.e., the Frobenius norm [3]. Latent Dirichlet Allocation (LDA) is a generative probabilistic model optimizing the posterior distribution of the topic assignment [4].
Both NMF and LDA require users to define the number of topics. How do we know how many topics we should put in the model? We can use a grid search and find the optimal parameters (topics, learning rate, etc.) that can optimize the log-likelihood value [5].</p>
<p>The output is a list of topics and their associated words, which can then be used to predict the topic of each file. However, since it is unsupervised, we cannot validate and iterate the model, and the performance of this prediction is often less than ideal. So instead of using the model to predict file topics, my goal for this step is to come up with a list of topics, so that we can use this list of topics to facilitate labeling.
For the implementations of N-gram analysis and topic modeling, check out <a class="reference external" href="https://sophiamyang.github.io/DS/text/text_basics.html">this article</a>.</p>
</div>
</div>
<div class="section" id="labeling">
<h2>Labeling<a class="headerlink" href="#labeling" title="Permalink to this headline">¶</a></h2>
<p>Now, we should have a list of topics produced from the topic modeling. We can then label our files to this list of topics.
If we have resources, we can hire third parties to do the labels. Otherwise, we can label the topics ourselves. We can even construct our labeling task as a survey, with dropdown choices from prepopulated topics. We will also include the “Other, please specify” option if none of the topics match with the file.
Topic modeling also predicts the topic for each file. Although it might not be the most accurate, we can still use this information to try choosing a balanced sample of topics to label.</p>
</div>
<div class="section" id="modeling">
<h2>Modeling<a class="headerlink" href="#modeling" title="Permalink to this headline">¶</a></h2>
<div class="section" id="machine-learning-model">
<h3>Machine learning model<a class="headerlink" href="#machine-learning-model" title="Permalink to this headline">¶</a></h3>
<p>We randomly sample our labeled data into training (70%), validation(20%), and testing (10%) datasets (the percentages depend on the sample size). The input of my machine learning model is the bag of words matrix for the text files. For any additional information of text (e.g., file source), we can dummy code this information and concatenate the dummies to our document-term matrix. The output is the labels.</p>
<p>I would like to test four machine learning models and compare their performances on model accuracy and confusion matrix through k-folds cross-validations.</p>
<ul class="simple">
<li><p>Multinomial logistic regression</p></li>
<li><p>Multinomial Naive Bayes</p></li>
<li><p>Support vector machine</p></li>
<li><p>XGBoost</p></li>
</ul>
<p>Multinomial logistic regression is the most basic and easiest to interpret. However, I suspect multinomial logistic regression would perform the worst. Multinomial Naive Bayes is another popular model for text classification that can give us a benchmark result. Support vector machine and XGBoost should provide us better results. We will need to do some hyperparameter tuning to find the best performing model. From my experience, with various hyperparameters and methods to avoid overfitting, XGBoost usually works the best. For implementation details, check out <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> and <code class="docutils literal notranslate"><span class="pre">xgboost</span></code> documentations.</p>
</div>
<div class="section" id="deep-learning-model">
<h3>Deep learning model<a class="headerlink" href="#deep-learning-model" title="Permalink to this headline">¶</a></h3>
<p>Three types of deep learning models are suited for NLP tasks — recurrent networks (LSTMs and GRUs), convolutional neural networks, and transformers. The recurrent network takes a long time and is harder to train, and not great for text classification tasks. The convolutional neural network is easy and fast to train, can take many layers, and outperform the recurrent network [6][7]. The transformers model is the state of the art method, however, I do not have much experience with transformers. Thus, I will only talk about the convolutional neural network.</p>
<p>The input of the deep learning model is the matrix of the word2vec embedding for each text file. Again, we randomly sample our labeled data into training (70%), validation (20%), and testing (10%) datasets. We can increase the percentage of our training set if our sample size is small.</p>
<p>We will randomly separate training data into different batches. The sample size for each input needs to be the sample for each batch. Thus, we need to find the longest input text and pad all the other input text to match the length of the longest text.</p>
<p>For constructing the deep learning model, we can add multiple layers of convolutional blocks and then feed the results into a softmax layer to get the classification result (see figure 1). Note that with text analysis, we need to make sure that our receptive field is as large as possible so that the network is looking at the entire length of the input text for each document. Thus, we use many layers and dilated convolutions to increase the receptive field. The code below shows an example code of a convolutional block (note that for text generation, we will need to use a causal convolutional block. But for text classification, many-layered dilated convolution will be fine). For each layer, we can increase the number of dilations by a factor of 2. Then we can tune and train the model and apply various techniques to prevent overfitting until we are satisfied with the model performance. And the final step is to predict all file labels with our trained deep learning model.</p>
<p>Now you have a high-level view of text classification. Enjoy!</p>
<script src="https://gist.github.com/sophiamyang/555a87b4ac68e41b7abc17579cb7ea60.js"></script>
<p><img alt="" src="../../_images/text_classification.png" />
<em>Figure 1. Illustration of a convolutional network for classification [7].</em></p>
<p>References:</p>
<p>[1] http://proceedings.mlr.press/v32/santos14.html</p>
<p>[2] https://arxiv.org/pdf/1508.06615.pdf</p>
<p>[3] https://scikit-learn.org/stable/modules/decomposition.html#nmf</p>
<p>[4] https://scikit-learn.org/stable/modules/decomposition.html#latentdirichletallocation</p>
<p>[5] https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/</p>
<p>[6] http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture11-convnets.pdf</p>
<p>[7] http://www.philkr.net/dl_class/lectures/sequence_modeling/04.pdf</p>
<p>By Sophia Yang on <a class="reference external" href="https://towardsdatascience.com/an-overview-of-text-classification-b1ec14db358c">November 18, 2020</a></p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./DS/text"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="text_basics.html" title="previous page">Text analysis basics in Python</a>
    <a class='right-next' id="next-link" href="../timeseries/timeseries.html" title="next page">Time Series Analysis</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Sophia Yang<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>