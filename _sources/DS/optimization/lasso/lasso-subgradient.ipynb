{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa000c52",
   "metadata": {},
   "source": [
    "# Lasso regression from¬†scratch\n",
    "*Subgradient method implementation in¬†Python*\n",
    "\n",
    "## Problem statement \n",
    "\n",
    "Let‚Äôs assume we have N people/observations, each person has M features. We are given:\n",
    "\n",
    "A matrix $ùëã$ is $\\mathbb{R}^{N\\times M}$. $X_{ij}$ represents person i with feature j.\n",
    "\n",
    "A vector $y$ is $\\mathbb{R}^{N}$. $y_{i}$ represents the output of person i.\n",
    "\n",
    "We expect y is a linear combination of features in X and we want to find out the relationship between X and y. If y is continous, we know that this is just a linear regression problem that tries to solve w for $y = Xw + b$. \n",
    "\n",
    "\n",
    "## Goal\n",
    "\n",
    "There are many approaches to solve the linear regression problem. The most basic approach Ordinary Least Squares tries to minimize $\\min_w \\|{Xw-y}\\|_2^2$. Ridge regression adds a L2 regularization term on the weights $\\min_w \\left[\\|{Xw-y}\\|_2^2 + \\alpha \\|{w}\\|_2^2\\right]$\n",
    "\n",
    "\n",
    "Lasso regression adds a L1 regularization term on the weights $\\min_w \\left[\\|{Xw-y}\\|_2^2 + \\alpha \\|{w}\\|_1 \\right]$. The first term $\\|{Xw-y}\\|_2^2$ is smooth, convex, and differentiable. The second term,  $\\|{w}\\|_1$ ,however, is not smooth and not differentiable. Therefore, we can't use gradient descent method on Lasso directly. There are lots of fun methods for minimizing this issue. This article will walk though the easiest subgradient method for solving this problem.\n",
    "\n",
    "## Subgradient method\n",
    "\n",
    "The easiest way to optimize Lasso regression is to use the subgradient method. What is subgradient? Here is an explanation from [wikipidia](https://en.wikipedia.org/wiki/Subderivative): \n",
    "\n",
    "*In mathematics, the subderivative, subgradient, and subdifferential generalize the derivative to convex functions which are not necessarily differentiable.*\n",
    "\n",
    "Let $ f(z) = |z| $ and let's calculate the subdifferential, all the subgradients, of f at w:\n",
    "\n",
    "$\\partial f(z) = \\begin{equation}\n",
    "    \\begin{cases}\n",
    "      -1 & \\text{if $z$ < 0}\\\\\n",
    "      1 & \\text{if $z$ > 0}\\\\\n",
    "      [-1,1] & \\text{if $z$ = 0}\n",
    "    \\end{cases}       \n",
    "\\end{equation}$\n",
    "\n",
    "We can see that $ sign(z) \\in \\partial f(z) $, sign(z) is one of the subgradient of f at z. Alternatively, for $z \\neq 0$, we use sign(z) as the subgradient; whereas for for $z = 0$, we use a random number selected between -1 and 1 as the subgradient. To simplify the illustrations, we simply use sign(z) as one of the subgradient of f at z for all $z$.\n",
    "\n",
    "\n",
    "Then for the L1 norm of vector w:\n",
    "\n",
    "$\n",
    "sign(w) =\n",
    "\\begin{pmatrix}\n",
    "sign(w_1)\\\\\n",
    "sign(w_2)\\\\\n",
    "...\n",
    "\\end{pmatrix} \\in \\partial \\|{w}\\|_1 = \\begin{pmatrix}\n",
    "\\partial |w_1|\\\\\n",
    "\\partial |w_2|\\\\\n",
    "...\n",
    "\\end{pmatrix} $\n",
    "\n",
    "Therefore, for the Lasso loss function: loss = $\\frac{1}{2}\\|{Xw-y}\\|_2^2 + \\alpha \\|{w}\\|_1$ (I added $\\frac{1}{2}$ here for easy calculations), the subgradient can be written as: $X^T(Xw-y) + \\alpha * sign(w)$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dab524f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "def loss_subgradient(X, w, y, alpha):\n",
    "    z = X @ w - y\n",
    "    # calculate loss \n",
    "    loss = 0.5 * np.linalg.norm(z)**2 + alpha * np.linalg.norm(w, ord=1)\n",
    "    # calculate subgradient\n",
    "    subgradient = X.T @ z + alpha * np.sign(w)\n",
    "    return loss, subgradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b9654ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_method(X, y, max_iter=1000, eta=0.1, alpha=0.1):\n",
    "    w = np.zeros(X.shape[1])\n",
    "    step = 0\n",
    "    step_lst = []\n",
    "    loss_lst = []\n",
    "    w_lst = []\n",
    "    while step < max_iter:\n",
    "        step += 1\n",
    "        loss, subgradient = loss_subgradient(X, w, y, alpha)\n",
    "        w -= eta * subgradient\n",
    "        step_lst.append(step)\n",
    "        w_lst.append(w)\n",
    "        loss_lst.append(loss)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'step': step_lst, \n",
    "        'loss': loss_lst\n",
    "    })\n",
    "    return df, w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a8a34979",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoSubgradient:\n",
    "    def fit(self, X, y):\n",
    "        self.loss_steps, self.w = subgradient_method(X, y)\n",
    "        \n",
    "    def loss_plot(self):\n",
    "        return self.loss_steps.plot(\n",
    "            x='step',\n",
    "            y='loss',\n",
    "            xlabel='step',\n",
    "            ylabel='loss'\n",
    "        )\n",
    "    def predict(self, H):\n",
    "        # H: new data\n",
    "        return H @ self.w\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb7d693",
   "metadata": {},
   "source": [
    "In addition to the subgradient method, there are many other ways to optimize the loss function of Lasso. For example, proximal gradient, accelerated proximal gradient, and Frank-Wolf. I am not going to go through all the optimization methods here. I will talk a little bit about the Frank-Wolfe algorithm next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57916bcf",
   "metadata": {},
   "source": [
    "## Frank-Wolf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a5036f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0068d7f",
   "metadata": {},
   "source": [
    "## Benefit of L1 regularization\n",
    "\n",
    "\n",
    "xxx resulting in more 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9376ee72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10bdd978",
   "metadata": {},
   "source": [
    "This article went through the basic subgradient method in solving Lasso regression. There are other more interesting methods such as proxima gradient method, accelerate proximal gradient method, and Frank-Wolfe method. I might write more about those different methods later. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
